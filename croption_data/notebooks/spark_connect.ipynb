{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-11T13:45:47.809059600Z",
     "start_time": "2024-03-11T13:45:47.557934600Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m317.0/317.0 MB\u001B[0m \u001B[31m6.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting py4j==0.10.9.7 (from pyspark)\r\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m200.5/200.5 kB\u001B[0m \u001B[31m6.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hBuilding wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=164429a9c3322ac962ca4c08ad0a8d5bdae29d5ce1536d1d42eb5b26bb2d03de\r\n",
      "  Stored in directory: /home/mark/.cache/pip/wheels/95/13/41/f7f135ee114175605fb4f0a89e7389f3742aa6c1e1a5bcb657\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: py4j, pyspark\r\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T07:43:23.889054700Z",
     "start_time": "2024-03-08T07:42:35.941858200Z"
    }
   },
   "id": "8fc9170d6e467551",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Creating Spark Connect session to local Spark server on port 15002\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mremote(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msc://1e9ee632f234:7077\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
      "File \u001B[0;32m~/miniconda3/envs/croptionday/lib/python3.11/site-packages/pyspark/sql/session.py:479\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    477\u001B[0m     os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSPARK_CONNECT_MODE_ENABLED\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    478\u001B[0m     opts[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.remote\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m url\n\u001B[0;32m--> 479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m RemoteSparkSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;28mmap\u001B[39m\u001B[38;5;241m=\u001B[39mopts)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n\u001B[1;32m    480\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSPARK_LOCAL_REMOTE\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39menviron:\n\u001B[1;32m    481\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msc://localhost\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/croptionday/lib/python3.11/site-packages/pyspark/sql/connect/session.py:221\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m session \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    220\u001B[0m         session \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate()\n\u001B[0;32m--> 221\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_options(session)\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m session\n",
      "File \u001B[0;32m~/miniconda3/envs/croptionday/lib/python3.11/site-packages/pyspark/sql/connect/session.py:183\u001B[0m, in \u001B[0;36mSparkSession.Builder._apply_options\u001B[0;34m(self, session)\u001B[0m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 183\u001B[0m         session\u001B[38;5;241m.\u001B[39mconf\u001B[38;5;241m.\u001B[39mset(k, v)\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    185\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[0;32m~/miniconda3/envs/croptionday/lib/python3.11/site-packages/pyspark/sql/connect/conf.py:41\u001B[0m, in \u001B[0;36mRuntimeConf.set\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m     39\u001B[0m op_set \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mSet(pairs\u001B[38;5;241m=\u001B[39m[proto\u001B[38;5;241m.\u001B[39mKeyValue(key\u001B[38;5;241m=\u001B[39mkey, value\u001B[38;5;241m=\u001B[39mvalue)])\n\u001B[1;32m     40\u001B[0m operation \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mConfigRequest\u001B[38;5;241m.\u001B[39mOperation(\u001B[38;5;28mset\u001B[39m\u001B[38;5;241m=\u001B[39mop_set)\n\u001B[0;32m---> 41\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mconfig(operation)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m warn \u001B[38;5;129;01min\u001B[39;00m result\u001B[38;5;241m.\u001B[39mwarnings:\n\u001B[1;32m     43\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(warn)\n",
      "File \u001B[0;32m~/miniconda3/envs/croptionday/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1368\u001B[0m, in \u001B[0;36mSparkConnectClient.config\u001B[0;34m(self, operation)\u001B[0m\n\u001B[1;32m   1366\u001B[0m req\u001B[38;5;241m.\u001B[39moperation\u001B[38;5;241m.\u001B[39mCopyFrom(operation)\n\u001B[1;32m   1367\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1368\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m attempt \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrying():\n\u001B[1;32m   1369\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m attempt:\n\u001B[1;32m   1370\u001B[0m             resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stub\u001B[38;5;241m.\u001B[39mConfig(req, metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_builder\u001B[38;5;241m.\u001B[39mmetadata())\n",
      "File \u001B[0;32m~/miniconda3/envs/croptionday/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1684\u001B[0m, in \u001B[0;36mRetrying.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1681\u001B[0m             backoff \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39muniform(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jitter)\n\u001B[1;32m   1683\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRetrying call after \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbackoff\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ms sleep\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1684\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sleep(backoff \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m1000.0\u001B[39m)\n\u001B[1;32m   1685\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m AttemptManager(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_can_retry, retry_state)\n\u001B[1;32m   1687\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m retry_state\u001B[38;5;241m.\u001B[39mdone():\n\u001B[1;32m   1688\u001B[0m     \u001B[38;5;66;03m# Exceeded number of retries, throw last exception we had\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating Spark Connect session to local Spark server on port 15002\n",
    "\n",
    "spark = SparkSession.builder.remote(\"sc://1e9ee632f234:7077\").getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T07:49:22.930296700Z",
     "start_time": "2024-03-08T07:47:08.528627500Z"
    }
   },
   "id": "13cacec8eff24e1e",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install pyarrow  grpcio-status grpcio pyspark\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-08T07:47:06.201007900Z"
    }
   },
   "id": "1587adbcded3af70",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPySparkRuntimeError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 8\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Create a SparkSession that connects to the Spark master running in Docker\u001B[39;00m\n\u001B[1;32m      4\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder \\\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLocalDockerSparkApp\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39mmaster(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msc://localhost:7077\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mconfig(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark.executor.memory\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1g\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
      "File \u001B[0;32m~/miniconda3/envs/croptionday/lib/python3.11/site-packages/pyspark/sql/session.py:497\u001B[0m, in \u001B[0;36mSparkSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    495\u001B[0m     sparkConf\u001B[38;5;241m.\u001B[39mset(key, value)\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# This SparkContext may be an existing one.\u001B[39;00m\n\u001B[0;32m--> 497\u001B[0m sc \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39mgetOrCreate(sparkConf)\n\u001B[1;32m    498\u001B[0m \u001B[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001B[39;00m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;66;03m# by all sessions.\u001B[39;00m\n\u001B[1;32m    500\u001B[0m session \u001B[38;5;241m=\u001B[39m SparkSession(sc, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_options)\n",
      "File \u001B[0;32m~/miniconda3/envs/croptionday/lib/python3.11/site-packages/pyspark/context.py:515\u001B[0m, in \u001B[0;36mSparkContext.getOrCreate\u001B[0;34m(cls, conf)\u001B[0m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    514\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 515\u001B[0m         SparkContext(conf\u001B[38;5;241m=\u001B[39mconf \u001B[38;5;129;01mor\u001B[39;00m SparkConf())\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    517\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\n",
      "File \u001B[0;32m~/miniconda3/envs/croptionday/lib/python3.11/site-packages/pyspark/context.py:201\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    197\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    199\u001B[0m     )\n\u001B[0;32m--> 201\u001B[0m SparkContext\u001B[38;5;241m.\u001B[39m_ensure_initialized(\u001B[38;5;28mself\u001B[39m, gateway\u001B[38;5;241m=\u001B[39mgateway, conf\u001B[38;5;241m=\u001B[39mconf)\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[1;32m    204\u001B[0m         master,\n\u001B[1;32m    205\u001B[0m         appName,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    215\u001B[0m         memory_profiler_cls,\n\u001B[1;32m    216\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/croptionday/lib/python3.11/site-packages/pyspark/context.py:436\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_gateway:\n\u001B[0;32m--> 436\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_gateway \u001B[38;5;241m=\u001B[39m gateway \u001B[38;5;129;01mor\u001B[39;00m launch_gateway(conf)\n\u001B[1;32m    437\u001B[0m         SparkContext\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_gateway\u001B[38;5;241m.\u001B[39mjvm\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m instance:\n",
      "File \u001B[0;32m~/miniconda3/envs/croptionday/lib/python3.11/site-packages/pyspark/java_gateway.py:107\u001B[0m, in \u001B[0;36mlaunch_gateway\u001B[0;34m(conf, popen_kwargs)\u001B[0m\n\u001B[1;32m    104\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.1\u001B[39m)\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(conn_info_file):\n\u001B[0;32m--> 107\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkRuntimeError(\n\u001B[1;32m    108\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJAVA_GATEWAY_EXITED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    109\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{},\n\u001B[1;32m    110\u001B[0m     )\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(conn_info_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m info:\n\u001B[1;32m    113\u001B[0m     gateway_port \u001B[38;5;241m=\u001B[39m read_int(info)\n",
      "\u001B[0;31mPySparkRuntimeError\u001B[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession that connects to the Spark master running in Docker\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LocalDockerSparkApp\") \\\n",
    "    .master(\"sc://localhost:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Example operation: Create a DataFrame and show its\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T08:01:54.533954400Z",
     "start_time": "2024-03-08T08:01:53.207948Z"
    }
   },
   "id": "791c3de64590adf8",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "[sudo] password for mark: \r\n"
     ]
    }
   ],
   "source": [
    "!sudo apt install default-jdk\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-08T08:13:04.461117700Z",
     "start_time": "2024-03-08T08:04:22.249488300Z"
    }
   },
   "id": "7bc5e59c62cd9b35",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "756678c80f5c5132"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
