# docker-compose.yml
version: "3.8"
services:
  zookeeper:
    restart: always
    image: docker.io/bitnami/zookeeper:latest
    ports:
      - "2181:2181"
    volumes:
      - "zookeeper-volume:/bitnami"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
  kafka:
    restart: always
    image: docker.io/bitnami/kafka:latest
    ports:
      - "9093:9093"
    volumes:
      - "kafka-volume:/bitnami"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=CLIENT
    depends_on:
      - zookeeper

  timescaledb:
      image: timescale/timescaledb:latest-pg14
      ports:
        - "5432:5432"
      environment:
        POSTGRES_USER: team  # Replace 'rootusername' with your desired superuser name
        POSTGRES_PASSWORD: Python123  # Replace 'rootpassword' with your desired password
      volumes:
        - ./data/timescale/db-data:/var/lib/postgresql/data

  mongo:
    container_name: mongo
    restart: always
    image: mongo:4.4
    environment:
      MONGO_INITDB_ROOT_USERNAME: team
      MONGO_INITDB_ROOT_PASSWORD: Python123
    env_file:
      - .env # configure mongo
    volumes:
      - ./data/mongo/db-data:/data/db # persist data even if container shuts down
    ports:
      - 27018:27017

  spark-master:
    image: bitnami/spark:latest
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./data/spark/db-data:/opt/spark
    environment:
      - SPARK_MODE=master

  spark-worker:
    image: bitnami/spark:latest
    ports:
      - "8081:8081" # Optionally expose different ports for each worker if needed
      - "7078:7078"
    volumes:
      - ./data/spark/db-data:/opt/spark
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master

  ubuntu:
    image: ubuntu:22.04
    volumes:
      - ./scripts:/workspace
    working_dir: /workspace
    command: /workspace/startup.sh
    depends_on:
      - spark-master

  jupyterlab:
    image: jupyter/pyspark-notebook
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - PYSPARK_PYTHON=python3
      - SPARK_OPTS=--packages io.delta:delta-core_2.12:x.x.x,org.apache.spark:spark-sql-kafka-0-10_2.12:x.x.x --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
#      - JUPYTER_TOKEN=''
#      - JUPYTER_PASSWORD=''
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''
    depends_on:
      - spark-master
volumes:
  kafka-volume:
  zookeeper-volume:
  timescaledb-data: